# Các xử lý vấn đề khác

### Khôi phục lại rabbitmq khi cluster bị split brain hoặc down toàn bộ

- B1: Stop 3 rabbit trên 3 node chạy rabbitmq của cụm Cloud tương ứng

```
docker stop rabbitmq
```

- B2: Chọn 1 rabbitmq để bật lên trước

```
rm /var/lib/docker/volumes/rabbitmq/_data/mnesia/ -rf
rm /var/lib/docker/volumes/rabbitmq/_data/mnesia/MnesiaCore* -rf
docker re\start rabbitmq
```

- B3: Theo dõi log rabbitmq 1 lúc, ok thì thực hiện tác động trên 2 con tiếp theo, cách nhau 10

```
tailf /var/lib/docker/volumes/kolla_logs/_data/rabbitmq/rabbit@hlc6fctl-2.log
```

### Trong trường hợp bị `PortNotFoundClient`

Cần xóa port trong database, sau đó evacutate hoặc migrate lại VM, rồi attach lại port, và reboot VM.

```
MariaDB [neutron]> select * from ipamallocations where ip_address="10.255.59.40" \G;
MariaDB [neutron]> delete from ipamallocations where ip_address="10.255.59.40";

MariaDB [neutron]> select * from ports where id="c3cf90e0-1b73-4703-8e08-607804acf081" \G;
MariaDB [neutron]> delete from ports where id="c3cf90e0-1b73-4703-8e08-607804acf081";

MariaDB [nova]> select * from virtual_interfaces where instance_uuid="714a9da9-e220-449a-9d19-4c0df8c1da07";
MariaDB [nova]> delete from virtual_interfaces where instance_uuid="714a9da9-e220-449a-9d19-4c0df8c1da07";
```

### Flush path rác dính LVM của host

Kiểm tra Open Count của path cần xóa
```
360060160f5264f000cfe7c5e7228a20d   253:3    0   201G  0 mpath
└─vg--u01-lv--u01                   253:4    0   201G  0 lvm
[root@hlc6fcom-78-200 ~]# dmsetup info 360060160f5264f000cfe7c5e7228a20d
Name:              360060160f5264f000cfe7c5e7228a20d
State:             ACTIVE
Read Ahead:        256
Tables present:    LIVE
Open count:        1
Event number:      4
Major, minor:      253, 3
Number of targets: 1
UUID: mpath-360060160f5264f000cfe7c5e7228a20d
```

Kiểm tra holders của path cầm xóa thông qua dm
```
[root@hlc6fcom-78-200 ~]# ls -la /dev/mapper/360060160f5264f000cfe7c5e7228a20d
lrwxrwxrwx 1 root root 7 Feb  2 18:37 /dev/mapper/360060160f5264f000cfe7c5e7228a20d -> ../dm-3
[root@hlc6fcom-78-200 ~]# ls -la /sys/block/dm-3/holders/
total 0
drwxr-xr-x 2 root root 0 Feb  5 13:24 .
drwxr-xr-x 8 root root 0 Jul 17  2020 ..
lrwxrwxrwx 1 root root 0 Jul 17  2020 dm-4 -> ../../dm-4
[root@hlc6fcom-78-200 ~]# cat /sys/block/dm-3/holders/dm-4/dm/uuid
LVM-zfzMT5CavUHFnd93gT1fcLXjdpFyG2uu6ajrx6gBfDcoU4IRN4OAu3QAiq1Zb9Yn

[root@hlc6fcom-78-200 ~]# dmsetup info /dev/dm-4
Name:              vg--u01-lv--u01
State:             ACTIVE
Read Ahead:        256
Tables present:    LIVE
Open count:        0
Event number:      0
Major, minor:      253, 4
Number of targets: 1
UUID: LVM-zfzMT5CavUHFnd93gT1fcLXjdpFyG2uu6ajrx6gBfDcoU4IRN4OAu3QAiq1Zb9Yn
```

Thực hiện xóa device mapper của LV rác và kiểm tra lại holders của mpath thì thấy không còn
```
[root@hlc6fcom-78-200 ~]# dmsetup remove -u LVM-zfzMT5CavUHFnd93gT1fcLXjdpFyG2uu6ajrx6gBfDcoU4IRN4OAu3QAiq1Zb9Yn
[root@hlc6fcom-78-200 ~]# ls -la /sys/block/dm-3/holders/
total 0
drwxr-xr-x 2 root root 0 Feb  5 13:25 .
drwxr-xr-x 8 root root 0 Feb  5 13:24 ..
```
Kiểm tra lại Open Count thì đã về 0, chứng tỏ không có cái gì đang hold nó nữa
```
[root@hlc6fcom-78-200 ~]# dmsetup info 360060160f5264f000cfe7c5e7228a20d
Name:              360060160f5264f000cfe7c5e7228a20d
State:             ACTIVE
Read Ahead:        256
Tables present:    LIVE
Open count:        0
Event number:      4
Major, minor:      253, 3
Number of targets: 1
UUID: mpath-360060160f5264f000cfe7c5e7228a20d
```

Thực hiện xóa mpath
```
[root@hlc6fcom-78-200 ~]# docker exec -it multipathd multipath -f 360060160f5264f000cfe7c5e7228a20d

[root@hlc6fcom-78-200 ~]# dmsetup info 360060160f5264f000cfe7c5e7228a20d
Device does not exist.
Command failed.
[root@hlc6fcom-78-200 ~]# lsblk  | grep 360060160f5264f000cfe7c5e7228a20d
```

Xóa device lable ứng với path đó
```
echo 1 > /sys/class/block/sdd/device/delete
echo 1 > /sys/class/block/sde/device/delete
echo 1 > /sys/class/block/sdf/device/delete
echo 1 > /sys/class/block/sdg/device/delete
```

Kiểm tra Open Count của path cần xóa
```
360060160f5264f000cfe7c5e7228a20d   253:3    0   201G  0 mpath
└─vg--u01-lv--u01                   253:4    0   201G  0 lvm
[root@hlc6fcom-78-200 ~]# dmsetup info 360060160f5264f000cfe7c5e7228a20d
Name:              360060160f5264f000cfe7c5e7228a20d
State:             ACTIVE
Read Ahead:        256
Tables present:    LIVE
Open count:        1
Event number:      4
Major, minor:      253, 3
Number of targets: 1
UUID: mpath-360060160f5264f000cfe7c5e7228a20d
```

Kiểm tra holders của path cầm xóa thông qua dm
```
[root@hlc6fcom-78-200 ~]# ls -la /dev/mapper/360060160f5264f000cfe7c5e7228a20d
lrwxrwxrwx 1 root root 7 Feb  2 18:37 /dev/mapper/360060160f5264f000cfe7c5e7228a20d -> ../dm-3
[root@hlc6fcom-78-200 ~]# ls -la /sys/block/dm-3/holders/
total 0
drwxr-xr-x 2 root root 0 Feb  5 13:24 .
drwxr-xr-x 8 root root 0 Jul 17  2020 ..
lrwxrwxrwx 1 root root 0 Jul 17  2020 dm-4 -> ../../dm-4
[root@hlc6fcom-78-200 ~]# cat /sys/block/dm-3/holders/dm-4/dm/uuid
LVM-zfzMT5CavUHFnd93gT1fcLXjdpFyG2uu6ajrx6gBfDcoU4IRN4OAu3QAiq1Zb9Yn

[root@hlc6fcom-78-200 ~]# dmsetup info /dev/dm-4
Name:              vg--u01-lv--u01
State:             ACTIVE
Read Ahead:        256
Tables present:    LIVE
Open count:        0
Event number:      0
Major, minor:      253, 4
Number of targets: 1
UUID: LVM-zfzMT5CavUHFnd93gT1fcLXjdpFyG2uu6ajrx6gBfDcoU4IRN4OAu3QAiq1Zb9Yn
```

Thực hiện xóa device mapper của LV rác và kiểm tra lại holders của mpath thì thấy không còn
```
[root@hlc6fcom-78-200 ~]# dmsetup remove -u LVM-zfzMT5CavUHFnd93gT1fcLXjdpFyG2uu6ajrx6gBfDcoU4IRN4OAu3QAiq1Zb9Yn
[root@hlc6fcom-78-200 ~]# ls -la /sys/block/dm-3/holders/
total 0
drwxr-xr-x 2 root root 0 Feb  5 13:25 .
drwxr-xr-x 8 root root 0 Feb  5 13:24 ..
```
Kiểm tra lại Open Count thì đã về 0, chứng tỏ không có cái gì đang hold nó nữa
```
[root@hlc6fcom-78-200 ~]# dmsetup info 360060160f5264f000cfe7c5e7228a20d
Name:              360060160f5264f000cfe7c5e7228a20d
State:             ACTIVE
Read Ahead:        256
Tables present:    LIVE
Open count:        0
Event number:      4
Major, minor:      253, 3
Number of targets: 1
UUID: mpath-360060160f5264f000cfe7c5e7228a20d
```

Thực hiện xóa mpath
```
[root@hlc6fcom-78-200 ~]# docker exec -it multipathd multipath -f 360060160f5264f000cfe7c5e7228a20d

[root@hlc6fcom-78-200 ~]# dmsetup info 360060160f5264f000cfe7c5e7228a20d
Device does not exist.
Command failed.
[root@hlc6fcom-78-200 ~]# lsblk  | grep 360060160f5264f000cfe7c5e7228a20d
```

Xóa device lable ứng với path đó
```
echo 1 > /sys/class/block/sdd/device/delete
echo 1 > /sys/class/block/sde/device/delete
echo 1 > /sys/class/block/sdf/device/delete
echo 1 > /sys/class/block/sdg/device/delete
```


### Lỗi PortBindingAlreadyExists khi Live Migrate

- Log lỗi ở nova-conductor

```
instance: c2ca45f0-f324-4485-96eb-9b79135e23e8] Binding failed for port c4dfcc62-73d2-46b0-8b89-f6fa8250977f and host VTP-OTH-COM-01.
Error: (409 {"NeutronError": {"message": "Binding for port c4dfcc62-73d2-46b0-8b89-f6fa8250977f on host VTP-OTH-COM-01 already exists.", "type": "PortBindingAlreadyExists", "detail": ""}})
```

- Check database: Các mappings có trạng thái INACTIVE là không cần thiết và gây ra hiện tượng lỗi mmigrate

```
MariaDB [neutron]> select * from ml2_port_bindings;
+--------------------------------------+----------------+----------+-----------+---------+--------------------------------------------------------------------------------------------------------------------------+----------+
| port_id                              | host           | vif_type | vnic_type | profile | vif_details                                                                                                              | status   |
+--------------------------------------+----------------+----------+-----------+---------+--------------------------------------------------------------------------------------------------------------------------+----------+
| 1b70ad18-aa60-432e-ac5b-b6ff55933ebb | VTP-OTH-COM-03 | ovs      | normal    |         | {"datapath_type": "system", "ovs_hybrid_plug": true, "bridge_name": "br-int", "port_filter": true, "connectivity": "l2"} | ACTIVE   |
| 72570c7b-ce00-4084-a24b-0a290a800fb9 | VTP-OTH-COM-07 | ovs      | normal    |         | {"datapath_type": "system", "ovs_hybrid_plug": true, "bridge_name": "br-int", "port_filter": true, "connectivity": "l2"} | ACTIVE   |
| 7f4e242f-3201-4243-bde8-2cefb44dbb73 | VTP-CLD-NET-01 | ovs      | normal    |         | {"datapath_type": "system", "ovs_hybrid_plug": true, "bridge_name": "br-int", "port_filter": true, "connectivity": "l2"} | ACTIVE   |
| 9637a8b2-5d48-4dfa-9a70-fd93d4f860da | VTP-OTH-COM-03 | ovs      | normal    |         | {"datapath_type": "system", "ovs_hybrid_plug": true, "bridge_name": "br-int", "port_filter": true, "connectivity": "l2"} | INACTIVE |
| 9637a8b2-5d48-4dfa-9a70-fd93d4f860da | VTP-OTH-COM-06 | ovs      | normal    |         | {"datapath_type": "system", "ovs_hybrid_plug": true, "bridge_name": "br-int", "port_filter": true, "connectivity": "l2"} | ACTIVE   |
| 9872c746-f73c-4e23-8a0d-f61bea71abad | VTP-OTH-COM-01 | ovs      | normal    |         | {"datapath_type": "system", "ovs_hybrid_plug": true, "bridge_name": "br-int", "port_filter": true, "connectivity": "l2"} | ACTIVE   |
| 9da7dde0-390e-4e4a-a60e-9a8d2eba2e6f | VTP-OTH-COM-04 | ovs      | normal    |         | {"datapath_type": "system", "ovs_hybrid_plug": true, "bridge_name": "br-int", "port_filter": true, "connectivity": "l2"} | ACTIVE   |
| ac58ccdf-7820-46c2-9e74-dfc8ea140d85 | VTP-CLD-NET-02 | ovs      | normal    |         | {"datapath_type": "system", "ovs_hybrid_plug": true, "bridge_name": "br-int", "port_filter": true, "connectivity": "l2"} | ACTIVE   |
| b39f17c6-6935-46ab-b217-84e1628ca5b2 | VTP-OTH-COM-02 | ovs      | normal    |         | {"datapath_type": "system", "ovs_hybrid_plug": true, "bridge_name": "br-int", "port_filter": true, "connectivity": "l2"} | ACTIVE   |
| c4dfcc62-73d2-46b0-8b89-f6fa8250977f | VTP-OTH-COM-01 | ovs      | normal    |         | {"datapath_type": "system", "ovs_hybrid_plug": true, "bridge_name": "br-int", "port_filter": true, "connectivity": "l2"} | INACTIVE |
| c4dfcc62-73d2-46b0-8b89-f6fa8250977f | VTP-OTH-COM-02 | ovs      | normal    |         | {"datapath_type": "system", "ovs_hybrid_plug": true, "bridge_name": "br-int", "port_filter": true, "connectivity": "l2"} | INACTIVE |
| c4dfcc62-73d2-46b0-8b89-f6fa8250977f | VTP-OTH-COM-06 | ovs      | normal    |         | {"datapath_type": "system", "ovs_hybrid_plug": true, "bridge_name": "br-int", "port_filter": true, "connectivity": "l2"} | ACTIVE   |
| db3fc6cc-5861-4658-ad1c-480d81aa8a01 | VTP-CLD-NET-02 | ovs      | normal    |         | {"datapath_type": "system", "ovs_hybrid_plug": true, "bridge_name": "br-int", "port_filter": true, "connectivity": "l2"} | ACTIVE   |
| e8368fb3-311c-4cc9-ad77-218c49128fe5 | VTP-CLD-NET-01 | ovs      | normal    |         | {"datapath_type": "system", "ovs_hybrid_plug": true, "bridge_name": "br-int", "port_filter": true, "connectivity": "l2"} | ACTIVE   |
| f5eea8d4-181e-40b9-a236-b056f55cf65e | VTP-OTH-COM-04 | ovs      | normal    |         | {"datapath_type": "system", "ovs_hybrid_plug": true, "bridge_name": "br-int", "port_filter": true, "connectivity": "l2"} | ACTIVE   |
+--------------------------------------+----------------+----------+-----------+---------+--------------------------------------------------------------------------------------------------------------------------+----------+
15 rows in set (0.001 sec)
```

- Giải pháp tạm thời: Xóa các dòng trên trong table ml2_port_bindings và xóa các dòng k cần trong ml2_port_biding_level

```
MariaDB [neutron]> delete from ml2_port_bindings where status="INACTIVE";

```

```
MariaDB [neutron]> select * from ml2_port_binding_levels;
+--------------------------------------+----------------+-------+-------------+--------------------------------------+
| port_id                              | host           | level | driver      | segment_id                           |
+--------------------------------------+----------------+-------+-------------+--------------------------------------+
| 1b70ad18-aa60-432e-ac5b-b6ff55933ebb | VTP-OTH-COM-03 |     0 | openvswitch | 440f7993-cacb-4ebd-bf46-3b68740a8d87 |
| 72570c7b-ce00-4084-a24b-0a290a800fb9 | VTP-OTH-COM-07 |     0 | openvswitch | 440f7993-cacb-4ebd-bf46-3b68740a8d87 |
| 7f4e242f-3201-4243-bde8-2cefb44dbb73 | VTP-CLD-NET-01 |     0 | openvswitch | 68fe8971-3e78-433d-88df-a2f5b51cc6ec |
| 9637a8b2-5d48-4dfa-9a70-fd93d4f860da | VTP-OTH-COM-03 |     0 | openvswitch | 440f7993-cacb-4ebd-bf46-3b68740a8d87 |
| 9637a8b2-5d48-4dfa-9a70-fd93d4f860da | VTP-OTH-COM-06 |     0 | openvswitch | 440f7993-cacb-4ebd-bf46-3b68740a8d87 |
| 9872c746-f73c-4e23-8a0d-f61bea71abad | VTP-OTH-COM-01 |     0 | openvswitch | 566a355b-303a-47ae-b671-325e2c810707 |
| 9da7dde0-390e-4e4a-a60e-9a8d2eba2e6f | VTP-OTH-COM-04 |     0 | openvswitch | 440f7993-cacb-4ebd-bf46-3b68740a8d87 |
| ac58ccdf-7820-46c2-9e74-dfc8ea140d85 | VTP-CLD-NET-02 |     0 | openvswitch | 68fe8971-3e78-433d-88df-a2f5b51cc6ec |
| b39f17c6-6935-46ab-b217-84e1628ca5b2 | VTP-OTH-COM-02 |     0 | openvswitch | 566a355b-303a-47ae-b671-325e2c810707 |
| c4dfcc62-73d2-46b0-8b89-f6fa8250977f | VTP-OTH-COM-01 |     0 | openvswitch | 440f7993-cacb-4ebd-bf46-3b68740a8d87 |
| c4dfcc62-73d2-46b0-8b89-f6fa8250977f | VTP-OTH-COM-02 |     0 | openvswitch | 440f7993-cacb-4ebd-bf46-3b68740a8d87 |
| c4dfcc62-73d2-46b0-8b89-f6fa8250977f | VTP-OTH-COM-06 |     0 | openvswitch | 440f7993-cacb-4ebd-bf46-3b68740a8d87 |
| db3fc6cc-5861-4658-ad1c-480d81aa8a01 | VTP-CLD-NET-02 |     0 | openvswitch | 440f7993-cacb-4ebd-bf46-3b68740a8d87 |
| e8368fb3-311c-4cc9-ad77-218c49128fe5 | VTP-CLD-NET-01 |     0 | openvswitch | 440f7993-cacb-4ebd-bf46-3b68740a8d87 |
| f5eea8d4-181e-40b9-a236-b056f55cf65e | VTP-OTH-COM-04 |     0 | openvswitch | 440f7993-cacb-4ebd-bf46-3b68740a8d87 |
+--------------------------------------+----------------+-------+-------------+--------------------------------------+
15 rows in set (0.000 sec)

MariaDB [neutron]> select * from ml2_port_binding_levels where port_id="c4dfcc62-73d2-46b0-8b89-f6fa8250977f";
+--------------------------------------+----------------+-------+-------------+--------------------------------------+
| port_id                              | host           | level | driver      | segment_id                           |
+--------------------------------------+----------------+-------+-------------+--------------------------------------+
| c4dfcc62-73d2-46b0-8b89-f6fa8250977f | VTP-OTH-COM-01 |     0 | openvswitch | 440f7993-cacb-4ebd-bf46-3b68740a8d87 |
| c4dfcc62-73d2-46b0-8b89-f6fa8250977f | VTP-OTH-COM-02 |     0 | openvswitch | 440f7993-cacb-4ebd-bf46-3b68740a8d87 |
| c4dfcc62-73d2-46b0-8b89-f6fa8250977f | VTP-OTH-COM-06 |     0 | openvswitch | 440f7993-cacb-4ebd-bf46-3b68740a8d87 |
+--------------------------------------+----------------+-------+-------------+--------------------------------------+
3 rows in set (0.000 sec)

MariaDB [neutron]> select * from ml2_port_binding_levels where port_id="c4dfcc62-73d2-46b0-8b89-f6fa8250977f" and host="VTP-OTH-COM-01";
+--------------------------------------+----------------+-------+-------------+--------------------------------------+
| port_id                              | host           | level | driver      | segment_id                           |
+--------------------------------------+----------------+-------+-------------+--------------------------------------+
| c4dfcc62-73d2-46b0-8b89-f6fa8250977f | VTP-OTH-COM-01 |     0 | openvswitch | 440f7993-cacb-4ebd-bf46-3b68740a8d87 |
+--------------------------------------+----------------+-------+-------------+--------------------------------------+
1 row in set (0.000 sec)

MariaDB [neutron]> delete from ml2_port_binding_levels where port_id="c4dfcc62-73d2-46b0-8b89-f6fa8250977f" and host="VTP-OTH-COM-01";
Query OK, 1 row affected (0.001 sec)

MariaDB [neutron]> delete from ml2_port_binding_levels where port_id="c4dfcc62-73d2-46b0-8b89-f6fa8250977f" and host="VTP-OTH-COM-02";
Query OK, 1 row affected (0.002 sec)

MariaDB [neutron]> select * from ml2_port_binding_levels where port_id="9637a8b2-5d48-4dfa-9a70-fd93d4f860da";
+--------------------------------------+----------------+-------+-------------+--------------------------------------+
| port_id                              | host           | level | driver      | segment_id                           |
+--------------------------------------+----------------+-------+-------------+--------------------------------------+
| 9637a8b2-5d48-4dfa-9a70-fd93d4f860da | VTP-OTH-COM-03 |     0 | openvswitch | 440f7993-cacb-4ebd-bf46-3b68740a8d87 |
| 9637a8b2-5d48-4dfa-9a70-fd93d4f860da | VTP-OTH-COM-06 |     0 | openvswitch | 440f7993-cacb-4ebd-bf46-3b68740a8d87 |
+--------------------------------------+----------------+-------+-------------+--------------------------------------+
2 rows in set (0.001 sec)

MariaDB [neutron]> select * from ml2_port_binding_levels where port_id="9637a8b2-5d48-4dfa-9a70-fd93d4f860da" and host=;
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near '' at line 1
MariaDB [neutron]> 
MariaDB [neutron]> select * from ml2_port_binding_levels where port_id="9637a8b2-5d48-4dfa-9a70-fd93d4f860da" and host="VTP-OTH-COM-03";
+--------------------------------------+----------------+-------+-------------+--------------------------------------+
| port_id                              | host           | level | driver      | segment_id                           |
+--------------------------------------+----------------+-------+-------------+--------------------------------------+
| 9637a8b2-5d48-4dfa-9a70-fd93d4f860da | VTP-OTH-COM-03 |     0 | openvswitch | 440f7993-cacb-4ebd-bf46-3b68740a8d87 |
+--------------------------------------+----------------+-------+-------------+--------------------------------------+
1 row in set (0.000 sec)

MariaDB [neutron]> delete from ml2_port_binding_levels where port_id="9637a8b2-5d48-4dfa-9a70-fd93d4f860da" and host="VTP-OTH-COM-03";
Query OK, 1 row affected (0.001 sec)

```

- Giải pháp tương lai với các câu lệnh: https://specs.openstack.org/openstack/neutron-specs/specs/pike/portbinding_information_for_nova.html

### Recover MariaDB Database

In case of Galera DB went down without proper shutdown procedure, then we can not start Galera Cluster normally.

Normal procedure for recover:

1. Finding the node with biggest seqno by:
- Start container or process in side container (`mysqld_safe`) with option `--wsrep-recover`
- cat `grastate.dat` e.g:

```bash
┬─[root@:/var/lib/docker/volumes/mariadb/_data]─[15:40:41]
╰─>$ cat grastate.dat 
# GALERA saved state
version: 2.1
uuid:    2407e895-ec81-11e8-a625-4f5141a9bc1f
seqno:   7498
cert_index:
```

2. Start the `mariadb` daemon in biggest seqno node by appending `safe_to_bootstrap: 1` to last line of `grastate.dat` and change `wsrep_cluster_address` to `gcomm://` in `galera.cnf` or `my.cnf`. Consider this node as temp primary/master node.

3. Start other node sequentially, wait for `SYNCED` (not `Donor/Desynced`) status of cluster (via `show status like 'wsrep_%';` or via mariadb log) e.g:

```bash
| wsrep_local_state_comment    | Synced    |
```

or via log:

```bash
2018-11-20 15:46:15 140670949893888 [Note] WSREP: 1.0 (controller22): State transfer from 0.0 (controller21) complete.
2018-11-20 15:46:15 140670949893888 [Note] WSREP: Shifting JOINER -> JOINED (TO: 21571)
2018-11-20 15:46:16 140670949893888 [Note] WSREP: Member 1.0 (controller22) synced with group.
2018-11-20 15:46:16 140670949893888 [Note] WSREP: Shifting JOINED -> SYNCED (TO: 21571)
2018-11-20 15:46:16 140660850748160 [Note] WSREP: Synchronized with group, ready for connections
```

4. Stop temp primary node, change `wsrep_cluster_address` to old value of peering addresses e.g.: `wsrep_cluster_address = gcomm://10.240.193.21:4567,10.240.193.22:4567,10.240.193.23:4567`

5. Start mariadb on temp primary node.


[1]. https://www.percona.com/blog/2014/09/01/galera-replication-how-to-recover-a-pxc-cluster/

### VM can NOT obtain DHCP IP

**2018_10_10**

After run following check list, I didn't see any problem.
- Check VM console log
- Set static IP --> OK --> DHCP problem
- Jump into `neutron_dhcp_agent container`:
  - check `/var/lib/neutron/dhcp/7888a8f3-2c7d-4a21-8354-dd1e9909baab/leases` --> not see IP of failure VM
  - check `/var/lib/neutron/dhcp/7888a8f3-2c7d-4a21-8354-dd1e9909baab/host` --> duplicated IP of failure VM --> remove duplicated line and restart VM --> OK

Extras step:

- Jump into neutron_dhcp_agent container:
  - `echo "" > /var/lib/neutron/dhcp/7888a8f3-2c7d-4a21-8354-dd1e9909baab/host` --> then restart `neutron_dhcp_agent` --> `host` file should be generated again

**2018_11_20**

- One more time with CongHM's VM --> do similar thing to fix that.
